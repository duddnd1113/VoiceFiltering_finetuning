#-----------------------------------------------------------------------------
# Í≤ΩÍ≥†Î¨∏ Ïïà Îú®Í≤å Ï≤òÎ¶¨
#-----------------------------------------------------------------------------
import logging
logging.getLogger().setLevel(logging.ERROR)
logging.getLogger().setLevel(logging.CRITICAL)

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

#-----------------------------------------------------------------------------

import os
import json
import torch
import soundfile as sf
import librosa
import numpy as np
from datetime import datetime

from src.model.modeling_enh import VoiceFilter
from src.model.configuration_voicefilter import VoiceFilterConfig


#-----------------------------------------------------------------------------
# 0. GPU Ï≤¥ÌÅ¨
#-----------------------------------------------------------------------------
use_gpu = torch.cuda.is_available()
device = torch.device("cuda" if use_gpu else "cpu")


#-----------------------------------------------------------------------------
# 1. HF inference-style WAV loader
#-----------------------------------------------------------------------------
def load_wav_hf(path, target_sr=16000):
    """HF inferenceÏôÄ ÎèôÏùºÌïú Î∞©ÏãùÏúºÎ°ú wav Î°úÎìú."""
    try:
        wav, sr = sf.read(path)
        if wav.ndim > 1:
            wav = wav.mean(axis=1)
        if sr != target_sr:
            wav = librosa.resample(wav, orig_sr=sr, target_sr=target_sr)
        return wav.astype(np.float32)
    except:
        wav, sr = librosa.load(path, sr=target_sr, mono=True)
        return wav.astype(np.float32)


#-----------------------------------------------------------------------------
# 2. Padding (HFÏôÄ ÎèôÏùº)
#-----------------------------------------------------------------------------
def pad_to_chunk(wav, chunk_size):
    rem = len(wav) % chunk_size
    if rem == 0:
        return wav
    pad_len = chunk_size - rem
    return np.concatenate([wav, np.zeros(pad_len, dtype=np.float32)])


#-----------------------------------------------------------------------------
# 3. HF-style xvector embedding
#-----------------------------------------------------------------------------
def cal_xvector_sincnet_embedding(xvector_model, ref_wav, sr=16000, max_length=5):
    chunk_len = max_length * sr
    chunks = []

    for i in range(0, len(ref_wav), chunk_len):
        w = ref_wav[i:i + chunk_len]
        if len(w) < chunk_len:
            w = np.concatenate([w, np.zeros(chunk_len - len(w))])
        chunks.append(w)

    chunks = torch.tensor(chunks, dtype=torch.float32).unsqueeze(1)
    if use_gpu:
        chunks = chunks.cuda()

    with torch.no_grad():
        emb = xvector_model(chunks)

    return emb.mean(dim=0).cpu()


#-----------------------------------------------------------------------------
# 4. Î°úÏª¨ Î™®Îç∏ Î°úÎçî (HF from_pretrained ÏôÑÎ≤Ω Ïû¨ÌòÑ)
#-----------------------------------------------------------------------------
def load_voicefilter_model_local():
    config_path = "pretrained/config.json"
    ckpt_path   = "pretrained/pytorch_model.bin"

    # config Î°úÎìú
    config = VoiceFilterConfig.from_pretrained(config_path)

    # Î™®Îç∏ ÏÉùÏÑ±
    model = VoiceFilter(config)

    # Í∞ÄÏ§ëÏπò Î°úÎìú
    state = torch.load(ckpt_path, map_location="cpu")
    missing, unexpected = model.load_state_dict(state, strict=False)

    # print("[Local Model Load] Missing:", missing)
    # print("[Local Model Load] Unexpected:", unexpected)
    print("\n=== Local ConVoiFilter Loaded ===")

    # inference mode
    model.eval()

    # xvector freeze
    model.xvector_model.eval()
    for p in model.xvector_model.parameters():
        p.requires_grad = False

    return model


#-----------------------------------------------------------------------------
# 5. Inference wrapper (do_enh Í∑∏ÎåÄÎ°ú)
#-----------------------------------------------------------------------------
def enhance_audio(model, mix_wav, ref_wav, sr=16000):
    chunk_size = model.wav_chunk_size

    mix_wav = pad_to_chunk(mix_wav, chunk_size)
    ref_wav = pad_to_chunk(ref_wav, chunk_size)

    mix_tensor = torch.tensor(mix_wav, dtype=torch.float32).to(device)
    ref_tensor = torch.tensor(ref_wav, dtype=torch.float32).to(device)

    # embedding
    with torch.no_grad():
        spk_emb = cal_xvector_sincnet_embedding(model.xvector_model,
                                                ref_tensor.cpu().numpy(),
                                                sr=sr)
        spk_emb = spk_emb.to(device)

    # enhancement
    with torch.no_grad():
        enhanced = model.do_enh(mix_tensor, spk_emb)

    return enhanced.cpu().numpy()


#-----------------------------------------------------------------------------
# 6. Main ‚Äì Test only
#-----------------------------------------------------------------------------
if __name__ == "__main__":

    # 1. Î™®Îç∏ Î°úÎìú
    model = load_voicefilter_model_local().to(device)

    # 2. ÌÖåÏä§Ìä∏ ÌååÏùº Î°úÎìú
    mix_path = "test_data/Ïπ®Ï∞©Îß®+ÌïúÎ°úÎ°ú.wav"
    ref_path = "test_data/ÌïúÎ°úÎ°ú ÌÉÄÍ≤ü.wav"

    mix_wav = load_wav_hf(mix_path)
    ref_wav = load_wav_hf(ref_path)

    # 3. ÏùåÏÑ± Ìñ•ÏÉÅ Ïã§Ìñâ
    enhanced_audio = enhance_audio(model, mix_wav, ref_wav, sr=16000)

    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    result_dir = f"./results/{timestamp}"
    os.makedirs(result_dir, exist_ok=True)

    out_path = os.path.join(result_dir, "enhanced_output.wav")
    sf.write(out_path, enhanced_audio, 16000)

    print(f"üéâ Done! Enhanced audio saved at:\n‚û°  {out_path}\n")